---
title: "701-knn-with-loadings"
author: "Mac Campbell"
date: "2/21/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#One thing that bothers me...
GWAS versus absolute allele frequencies. Some sites may be discounted, presumably due to sample size.
Let's get the loadings from a DAPC and see what I can do with those.

contig_103_481573    
contig_58_571477   
contig_58_2863421   
contig_105394_1469   

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(randomForest)
library(ggvis)
library(class)
library(gmodels)
library(viridis)
library(caret)

phenos <- read_tsv("phenos/202.phenos", col_names=FALSE) %>% rename(Pop=X1) %>% mutate(Phenotype=Pop) %>% mutate(Variant=ifelse(Pop==0, "FWR", "MIG"))



genos<-read_tsv("outputs/300/202.geno.gz", col_names = FALSE) %>% mutate(Site=paste(X1,X2,sep="-")) 
genos<-genos %>% filter(Site %in% c("103-481573","58-571477","58-2863421"))
#Omitting this site because it caused very unusual k selection results                       
 #,"105394-1469"))   

#Using 66-2522582 from GWAS
#genos<-genos %>% filter(Site %in% c("66-2522582","58-571477","58-2863421"))               

nrow(genos)

genos<-genos %>% select(-Site)
genos<- genos %>% select(-X205)
genos[genos < 0] <- NA
trans<-as_tibble(t(genos))
trans<-trans[-1:-2,]
trans<-na.roughfix(trans)
```

Creating a training set: 
```{r, warning=FALSE, message=FALSE}
smeltTrain<-trans[1:122,]
train.labels<-phenos$Variant[1:122]
```

Naively, I can choose k for KNN from sqrt(N), here 61 is our largest class
```{r, warning=FALSE, message=FALSE}
smeltPred <- knn(train = smeltTrain , test = trans, cl = train.labels, k=sqrt(61))

phenos$Prediction <- smeltPred
#phenos$Probability <-round(attr(smeltPred, "prob"),2)

#How did it do?
CrossTable(x = phenos$Phenotype, y = phenos$Prediction, prop.chisq=FALSE)
```
#Optimizing K

Our I can get a bit more sophisticated about choosing K.
```{r, warning=FALSE, message=FALSE}

trControl <- trainControl(method = "repeatedcv",
                          number = 100,
                          repeats = 10)

training<-smeltTrain
training$Type<-train.labels

fit <- train(Type ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = seq(from=1, to=71, by=2)),
             trControl  = trControl,
             metric     = "Accuracy",
             data       = training)
#Note, this chose 21 the first time I tried it
pred <- knn(train = smeltTrain, test = trans, cl = train.labels, k=fit$finalModel$k, prob = TRUE)

phenos$OptKPred<-pred
phenos$OptKProb<-round(attr(pred, "prob"),2)
CrossTable(x = phenos$Phenotype, y = phenos$OptKPred, prop.chisq=FALSE)

```

Let's not forget to look at the fit and k
```{r, warning=FALSE, message=FALSE}
fit$finalModel$k

ggplot(fit)
```

Ahh, our accuracy is definitely peaking at ~ 0.7, and the indicated K is appropriate I think.

