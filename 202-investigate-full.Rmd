---
title: "202-investigate-full"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Taking a step back. What kind of things should I be looking at?

__1__ How good are the data?
__2__ What signals are in the data?
__3__ What about differences between otolith calls?


## Part 1

Let's check our samples for average coverage. On the farm, maybe I can do a one liner (this way isn't the fastest).   
```{sh, eval=FALSE}
srun --partition=high --nodes=1 --time=1-01:00:00 cat bamlists/full.bamlist | while read line; do samtools depth $line | awk '{sum+=$3} END { print sum/NR}' >> outputs/200/full.coverage; done;
```

Now we can look at the distribution of coverage in our samples, and drop the bottom fraction:    

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
samples<-read_tsv("outputs/100/meta2.rda")
bams<-read_tsv("bamlists/full.bamlist",col_names = FALSE) %>% mutate(`Sequence File Name`=gsub(".sort-n.fixmate-m.sort.markdup-r.bam","",X1)) %>% mutate(Bams=X1)
bams$`Sequence File Name`<-gsub("bams/","",bams$`Sequence File Name`)

coverage<-read_tsv("outputs/200/full.coverage", col_names = FALSE)

combined<-inner_join(bams,samples)

combined$Coverage<-coverage$X1

ggplot(combined)+
  geom_histogram(aes(x=Coverage))+
  facet_grid(.~Phenotype)+
  theme_bw()+
  theme(panel.grid = element_blank())+
  xlim(0,20)

filtered<- combined %>% top_frac(.85, Coverage)

ggplot(filtered)+
  geom_histogram(aes(x=Coverage))+
  facet_grid(.~Phenotype)+
  theme_bw()+
  theme(panel.grid = element_blank())+
  xlim(0,20)

filtered %>% select(Phenotype) %>% group_by(Phenotype) %>% summarize(Count=n())
```

Let's replug the association test with these filtered data. Need a .bamlist and a .phenotype file.

```{r, warning=FALSE, message=FALSE}
bams<-select(filtered, `Sequence File Name`) %>% mutate(Path=paste0("bams/",`Sequence File Name`,".sort-n.fixmate-m.sort.markdup-r.bam")) %>% select(Path)
write_tsv(bams, path = "bamlists/partial.bamlist", col_names = FALSE)

phenos<-select(filtered,Phenotype)
phenos$Phenotype<-gsub("FWR",0,phenos$Phenotype)
phenos$Phenotype<-gsub("MIG",1,phenos$Phenotype)
write_tsv(phenos, path = "phenos/partial.phenos", col_names=FALSE)
```

Running 202.1-partial-asso.sh on cluster. Did we get any hits?    
```{r, warning=FALSE, message=FALSE}
data<-read_tsv(file="outputs/200/partial.lrt0.gz")

#Converting to log10p
data <- data %>% mutate(log10p = -log10(dchisq(LRT, df = 1))) %>%
  filter(log10p > 2)

#Check for any significant values
data %>% filter(log10p > 4)
```


## Part 2

There are other options to association tests. First off, I will make a DAPC plot of the two phenotypes and examine the loadings. I can also look a PCA to see some things about the data.   I'll make a *.vcf so I can also prune it later for linked SNPs.   

Runing 202.2-make-vch.sh on cluster. Seg fault again. Switching to making text plink files to convert to .vcf. We can recode and prune linked SNPs.  Plink doesn't like non-human and numerically coded chromosome names, so I'll label them as contig_dd.

```{sh, eval=FALSE}
#in outputs/200
cat partial-vcf.tped | perl -pe 's/^(\d+)\s\d+_/contig_$1 contig_$1_/' > temp.tped
cp partial-vcf.tfam temp.tfam

plink --tped temp.tped --tfam temp.tfam  --out binary --recode --allow-extra-chr --noweb
plink --ped binary.ped --map binary.map --recode vcf --allow-extra-chr -out recode
bcftools +prune -l 0.9 -w 10000 recode.vcf  -Ov -o recode.prune.vcf
```

4719 SNPs!    

```{r,eval=FALSE}
library(adegenet)
library(vcfR)
vcf<-read.vcfR(file="outputs/200/recode.vcf")
genind<-vcfR2genind(vcf)
save(genind, file="outputs/200/recode.genind")
```

```{r, warning=FALSE, message=FALSE}
load("outputs/200/recode.genind")
genind@pop=as.factor(filtered$Phenotype)
dapc<-dapc(genind, n.pca=175, n.da=1)
scatter(dapc)

contrib <- loadingplot(dapc$var.contr, axis=1,
thres=.0015, lab.jitter=1)
```

Considering panmictic population, perhaps looking a a machine learning approach to get at if variants can be used to assign fish back to phenotypes may work.  